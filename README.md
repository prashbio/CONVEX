Authors: Ujjwal Singh, Ranjana M Raju, Dr. Prashanth N SurAvajhala

System Genomics Lab, Amrita School of Biotechnology, Kollam, Kerala
# Validated Variant Detection Program to find Causal Pathogenicity 
The detection of the germline variants using Illumina sequencing of human DNA is not an easy task. Multiple approaches have been proposed; however, all the methods have their limitations. We present a pipeline that aids the research in complex variant detection. By using consensus meta-approach, 4 independent variant callers are being used to identify a uniform set of high-quality variants. These set of variants are then compared with the variant record of ClinVar database and finally, files containing pathogenic, and variants of uncertain significance are reported. The pipeline works using raw sequencing data and performs all the necessary steps automatically, significantly reducing the researchersâ€™ time required for processing the data and provides ease in finding corresponding validated variants under the scope. The output files contain single nucleotide polymorphisms, indels, consensus set of variants, and validated ClinVar record for the same. We strongly believe that this approach would be very valuable to the scientific community involved in variant studies. Considering the high cost of processing the reads for the variant studies, this automation can significantly reduce monetary, as well as the tremendous hard work necessary without compromising on the quality of variants reported.

Next-generation sequencing (NGS) has revolutionized the field of genomics by enabling the rapid and cost-effective sequencing of large amounts of DNA. NGS has numerous applications, including genome sequencing, transcriptome analysis, and epigenetics. Emerging technologies in the field of genomics, such as long-read sequencing and single-cell sequencing, are advancing the capabilities of NGS and expanding its potential applications.
NGS technologies, such as Illumina sequencing, rely on the generation of short DNA fragments, which are then sequenced in parallel. The resulting sequence reads are then aligned to a reference genome or assembled de novo to reconstruct the original sequence. The advantages of NGS include high throughput, high accuracy, and relatively low cost per base. These characteristics have made NGS a popular tool for a wide range of applications, including population genetics, cancer research, and microbiome analysis.

![image](https://github.com/pup98/Variant-Detection/assets/99062865/af08f803-2f64-4b67-83ba-f4b6362c8bbd)

All the protein coding sequences in the human genome can be analyzed thanks to whole exome sequencing (WES). This technology makes it possible to look at genetic abnormalities that are mostly found in the exonic regions of cancerous cells. WES offers cost-effective high-throughput outcomes. Here, we examine the analysis software that enables the use of WES data in clinical and academic settings. Technically, WES enables the initial detection of copy number variations (CNVs) and single nucleotide variants (SNVs), and the data obtained using both techniques can be merged and used in other ways. SNV variant calling pipelines can be machine learning-based combination pipelines or standalone solutions. The number of reads aligned to a specific region is compared by CNV detection tools. SNVs and CNVs both aid in the detection of mutations that cause druggable changes in pharmacology

## Proposed Pipeline 
We have been approaching this variant calling aspect with a different perspective, which origins at the initial step of NGS data analysis workflow that is at the sequencing step. Usually, to get a highly quality bonafidae variants we need to perform sequencing of sample at a high coverage optimum. Which leads to high cost of run per sample, this can be tackled using an alternative approach that involves using multiple variant callers to identify the variants and we cheery pick those consensus variants. This alternative method would also lead to the same high confidence list of variants, though wo would save a ton as a low coverage output will also suffice for the same study. Keeping this idea in mind, we have developed a variant calling pipeline which takes raw reads as inputs and uses 4 variant callers to find the inherent variants, the resulting vcf file is then passed on to a script which we designed that gives us the final list of high confidence variants that can be put to downstream analysis. 

![image](https://github.com/pup98/Variant-Detection/assets/99062865/157b0fee-ac11-48c9-b6ac-f35d17c9f25a)

Below is the proposed pipeline: 

![image](https://github.com/pup98/Variant-Detection/assets/99062865/78cb067d-b3a3-4101-a7e8-4cdc9a30051b)

### Variant Calling Softwares Used
Numerous aligners and variant callers have been developed and combined into various pipelines in order to call variations from this NGS data. An aligner and a variant caller are typically found in a pipeline; the aligner maps the sequencing reads to a reference genome, while the variant caller finds variant sites and determines the subject's genotype (s). There has been a lot of research done on the effectiveness of various aligners. Because alignment is a string-matching procedure and BWT-based aligners exploit data compression features by building an index of the reference genome to aid string matching, they are often quick and memory efficient. The Burrows-Wheeler Aligner (BWA) exhibits a favourable trade-off between processing time, memory consumption, and precision. For a systematic comparison, we chose BWA as the common aligner for all the variant callers and calling strategies which we would implement (Lie et al, 2013).
Here we have used following variant callers: 
1.	Varscan: 
Varscan is a platform independent software tool developed at the Genome Institute at Washington University to detect variants in NGS data. It houses mutation calling capabilities for targeted, exome, and whole-genome sequencing data generated on Illumina, SOLiD, Life/PGM, Roche/454 and similar instruments. A probabilistic framework, such as Bayesian statistics, is used by most published variant callers for next-generation sequencing data to identify variants and rate their confidence. These methods typically function pretty well, but there are a number of variables that can cause confusion, including pooled samples, excessive read depths, and contaminated or impure samples. The robust heuristic/statistical technique used by Varscan, in contrast, identifies variations that satisfy predetermined limits for read depth, base quality, variant allele frequency, and statistical significance. It takes SAMtools mpileup input to call variants. It offers lot of options to work around variants for e.g.: 
A.	Germline variants (SNP/Indel) in individual samples or pool of samples
B.	Multi-sample variants (shared or private) in multiple samples datasets
C.	Somatic mutations
D.	Somatic copy number alterations

2.	SAMtools/BCFtools:
is a collection of tools for working with variant calls in the Variant Call Format (VCF) and its binary equivalent, BCF. It can filter, query, convert, and validate VCF and BCF files, among other things. Variant calling, one of the core features of BCFtools, uses a two-step process to call variations. Using BCFtools mpileup, it first creates a pileup file. The per base coverage is compiled in the pileup file. Next, calls to variations from the pileup file are made using BCFtools call. It utilizes maximum likelihood approach and uses baseayan framework and applies statistical models to estimate likelihood of variant in each genomic position. 
3.	VT:
is a variant tool set that discovers short variants from NGS data. This tool is used for the manipulation, annotation, selection, and simulation, and analysis of variants in the context of next gen sequencing analysis. Vt is a variant tool that can manipulate VCF file by indexing, sorting, normalization, decompose biallelic block substitutions and also multiallelic variants. Though it takes in a raw sorted bam file as an input and is little different compared to other callers. 
4.	FreeBayes:
is a Bayesian genetic variant detector that leverages population-wide information to accurately identify genetic variants from next-generation sequencing (NGS) data. Developed by Erik Garrison and Gabor Marth at the Boston College Genome Sequencing and Analysis Core, FreeBayes provides an efficient and flexible solution for variant calling. It is written in C++ and implements a haplotype-based approach to detect single nucleotide variants (SNVs), insertions, deletions, and other types of genetic variants. FreeBayes employs a probabilistic framework, utilizing Bayes' theorem to estimate the likelihood of genetic variants given the observed sequencing data. This approach enables reliable variant detection, especially in regions of low coverage or complex genomic landscapes (Garrison and Marth, 2012).

## In a nut shell
* Above is the proposed pipeline, that we have designed to accomplish the stated objective. The complete workflow is a bash script that takes in paired ended reads (forward and reverse) as input and outputs various found variant categories after mapping to the ClinVar variant database. At first these reads are aligned to the reference sequence (we have used hg38 as the human reference guide) using bowtie aligner tool. Bowtie 2 is an incredibly quick and memory-effective tool for matching sequencing reads to lengthy reference sequences. It is particularly adept at aligning reads that range in length from 50 to 100 or 1,000 characters, as well as reads that are relatively large (such as mammalian genomes). To minimise memory usage, Bowtie 2 uses an FM Index to index the genome; for the human genome, this results in a memory footprint of about 3.2 GB. Gapped, local, and paired-end alignment modes are supported by Bowtie 2 and the output is stored in a SAM file. Heng Li, Bob Handsaker, and others created the text-based Sequence Alignment Map (SAM) format to store biological sequences that have been aligned to a reference sequence. It was created after the 1000 Genomes Project made the decision to create a new format and wished to move away from the MAQ mapper format. The format's overall TAB-delimited flavour originated from an earlier format that was influenced by BLAT's PSL. Gabor Marth from the University of Utah first developed a format with the same name but a different syntax that was more akin to a BLAST output. This is where SAM gets its name. It is commonly used for storing data produced by next-generation sequencing technologies, such as nucleotide sequences, and the standard has been expanded to allow unmapped data. The format supports short and long reads (up to 128 Mbp) produced by different sequencing platforms and is used to hold mapped data.
* A BAM file is the name of the compressed binary form of SAM. We utilise this version to make the file smaller and to enable indexing, which allows for quick random access to the data it contains. Simply put, a BAM file is a compressed SAM file. There is an optional header at the top of the file. Depending on the aligner being used, the header will change to describe the data source, reference sequence, alignment method, etc. The header is followed by the alignment section. The alignment data for a single read is contained in each of the lines that follow. There are 11 necessary fields for crucial mapping information on each alignment line, and there may be additional fields for aligner-specific data. While a sorted BAM has the data sorted by chromosomes/contigs/scaffolds whatever is in your reference genome. In order to efficiently display/access data the BAM file has to be sorted before performing any analysis.
* A text-based format called "pileup" is used to condense the base calls of reads that have been aligned to a reference sequence. This style makes SNP/indel calling and alignment more easily visible. Tony Cox and Zemin Ning utilised it for the first time at the Wellcome Trust Sanger Institute, and after it was integrated into the SAMtools software suite, it gained widespread recognition. Therefore, we use SAMtools suite for these conversions, either SAM/BAM or BAM/pileup format. 
* Next, we use this condensed sequence information in BAM and pileup formats as inputs to call the SNPâ€™s and INDELâ€™s taking help of variant calling softwares. In the pipeline we have used Varscan and BCFtools that take in mpileup format as input to call the variants, while FreeBayes and VT directly take in sorted bam as input to do the same.  In result we get 4 VCF files corresponding to the different variant callers that we have used. Later, these 4 VCF files are subjected to a consensus pipeline to find the common records among all. Idea behind finding consensus among them works on the premise of string manipulation and filtering: In the first step common chromosome number and loci are filtered and in second step we separate out common records in alternate allele column. The result of the above stated steps is a VCF file that harbours the absolute common variants shared by all the variant callers. 
* Until now we have the information about the variants shared by all the 4 VCF files. But, for a complete downstream analysis in order to pinpoint the pathogenicity, we need to tag the variants with the corresponding information. To achieve this, ClinVar variant database was used, ClinVar is a public archive where reports on the connections between phenotypes and human variants are available for free. Germline and somatic variations of any size, type, or genomic position are included in the database. Clinical testing facilities, research facilities, locus-specific databases, UniProt, expert panels, and practical recommendations contribute interpretations. It is a validated repository of genomic variations and is usually updated every 4, 5 months and gathers trust of geneticist, and biologists around the globe. We downloaded, the ClinVar database and ran a string search of chromosome number and position for our consensus variant file against the database. The output of this operation is cumulative chromosome position, variant information and pathogenicity tag for each record. From this output we filter 4 tags corresponding to pathogenicity: Pathogenic, likely_pathogenic, Pathogenic/Likely_pathogenic, Uncertain_significance and Conflicting _interpretations_of_pathogenicity and save each in a file named accordingly. 

## Requirements
* The script is written in bash, so you need a linux environment to run it. 
* You can use any genome alligner you prefer, I have used Bowtie2. Please install the binaries of preferred alligner, you can use BWA, Tophat.
* Install binaries of following variant callers: Varscan, Samtools, VT, freebayes
* Download the ClinVar database in vcf format: https://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh38/clinvar_20230611.vcf.gz
* Make sure to enter the correct path of all the software binaries installed for program to successfully give results. 

